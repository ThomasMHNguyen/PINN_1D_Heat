{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Network for 1D Diffusion Equation\n",
    "\n",
    "\n",
    "This notebook will be using a Physics-Informed Neural Network to solve the 1-D Diffusion/Heat Equation. Suppose we have the Diffusion Equation:\n",
    "\n",
    "$$ \\frac{\\partial{U}}{\\partial{t}} = D\\frac{\\partial^{2}{U}}{\\partial{x^{2}}},$$\n",
    "\n",
    "with the Dirichlet boundary conditions:\n",
    "\n",
    "$$ U\\big|_{x = -L,t } = 0,$$\n",
    "$$ U\\big|_{x = L,t } = 0,$$\n",
    "\n",
    "and the following initial condition:\n",
    "\n",
    "$$ U\\big|_{x,t =0} = \\sin(\\pi x).$$\n",
    "\n",
    "To simplify the system, we shall set $D=1$ and $L=1$. Additionally, we shall choose the domains of our system such that we know the analytical solution to the PDE. As such, our domains are:\n",
    "\n",
    "$$x \\in (-1,1)$$\n",
    "\n",
    "$$t \\in (0,1)$$\n",
    "\n",
    "Because, of these parameters, the exact solution to the PDE is $U = e^{-t} \\sin(\\pi x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, let's import our needed Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time, logging, sys, os\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "\n",
    "# Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "# Set the device, if CUDA is installed we use the GPU, otherwise we use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore the following lines of code for now; these are functions to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(x, t, y, folder=\"\", name=\"\", show=None, error=0, loss=0, iter=0):\n",
    "    \"\"\"\n",
    "    This function plots U(x,t) at every iteration to show the solution to the PDE. It will also plot the NN Loss and relative error determined by the L^2 norm.\n",
    "    \"\"\"\n",
    "    X, T = x.cpu(), t.cpu()\n",
    "    F_xt = y.cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, dpi=200)\n",
    "    cp = ax.contourf(X, T, F_xt, 20, cmap=\"rainbow\")\n",
    "    fig.colorbar(cp)  # Add a colorbar to a plot\n",
    "    ax.set_xlabel(\"x\")\n",
    "    # ax.set_ylabel(\"x2\")\n",
    "    ax.set_ylabel(\"t\")\n",
    "    if error > 0 and loss != 0:\n",
    "        ax.set_title(f\"Iteration: {iter}, Loss: {loss:.5f} \\n  Relative error: {error:.5f}\")\n",
    "    elif error > 0:\n",
    "        ax.set_title(f\"Iteration: {iter} \\n Relative error: {error:.5f}\")\n",
    "    elif loss != 0:\n",
    "        ax.set_title(f\"Iteration: {iter}, Loss: {loss:.5f}\")\n",
    "    else:\n",
    "        ax.set_title(f\"Iteration: {iter}\")\n",
    "    plt.savefig(f\"{folder}/contour_{name}.png\")  # Save the first image as an image file\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(dpi=200)\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    ax.plot_surface(X.numpy(), T.numpy(), F_xt.numpy(), cmap=\"rainbow\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"t\")\n",
    "    \n",
    "    if error > 0 and loss != 0:\n",
    "        ax.set_title(f\"Iteration: {iter}, Loss: {loss:.5f} \\n  Relative error: {error:.5f}\")\n",
    "    elif error > 0:\n",
    "        ax.set_title(f\"Iteration: {iter} \\n Relative error: {error:.5f}\")\n",
    "    elif loss != 0:\n",
    "        ax.set_title(f\"Iteration: {iter}, Loss: {loss:.5f}\")\n",
    "    else:\n",
    "        ax.set_title(f\"Iteration: {iter}\")\n",
    "        \n",
    "    ax.set_zlim3d(-1, 1) \n",
    "\n",
    "        \n",
    "    plt.savefig(f\"{folder}/{name}.png\")  # Save the first image as an image file\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(test_folder, count, remove_imgs=True):\n",
    "    \"\"\"\n",
    "    This function generates the gif that plots U(x,t) over the course of the training/testing stage.\n",
    "    \"\"\"\n",
    "    n = count  # The number of pictures to be stitched into a video\n",
    "    fps = n // 5  # The frame rate of the generated video\n",
    "    imgs = []\n",
    "    for i in range(1, n, 1):\n",
    "        img_file = f\"{test_folder}/approx_{i}.png\"\n",
    "        img = imageio.imread(img_file)\n",
    "        img = Image.fromarray(img)  # .resize((size_width, size_height))\n",
    "        imgs.append(img)\n",
    "        os.remove(img_file) if remove_imgs else None\n",
    "    filename1 = os.path.join(test_folder, \"0_video_u.mp4\")\n",
    "    filename2 = os.path.join(test_folder, \"0_video_u.gif\")\n",
    "    imageio.mimwrite(filename1, imgs, fps=fps)  #\n",
    "    imageio.mimsave(filename2, imgs, format='GIF')\n",
    "\n",
    "    imgs = []\n",
    "    for i in range(1, n, 1):\n",
    "        img_file = f\"{test_folder}/contour_approx_{i}.png\"\n",
    "        img = imageio.imread(img_file)\n",
    "        img = Image.fromarray(img)  # .resize((size_width, size_height))\n",
    "        imgs.append(img)\n",
    "        os.remove(img_file) if remove_imgs else None\n",
    "    filename = os.path.join(test_folder, \"0_video_u_contour.mp4\")\n",
    "    imageio.mimwrite(filename, imgs, fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following set of functions will generate the PyTorch tensors needed for the neural-network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pde_data(x_min,x_max,t_min,t_max,N_train_x,N_train_t):\n",
    "    \"\"\"\n",
    "    This function creates the PyTorch arrays needed for the PINN. The arrays contain all possible combinations of x,t arranged into a 2D array.\n",
    "    \"\"\"\n",
    "    # Determine why omit first and last values #\n",
    "    x = torch.linspace(x_min, x_max, N_train_x)\n",
    "    t = torch.linspace(t_min, t_max, N_train_t)\n",
    "    X, T = torch.meshgrid(x, t)\n",
    "\n",
    "    # x values are up/down column 1, t values are up/down column 2\n",
    "    # Note: in order to stack torch tensors, tensors must be 2D unlike numpy arrays\n",
    "    X_pde = torch.hstack((X.transpose(1, 0).flatten().view(-1, 1), T.transpose(1, 0).flatten().view(-1, 1)))\n",
    "    return X_pde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are arranging our meshgrids of X and T as such:\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/grid_schematic.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ic_bc_data(x_min,x_max,t_min,t_max,N_bc):\n",
    "    \"\"\"\n",
    "    This function creates the PyTorch tensor that are modified for the initial condition\n",
    "    and the boundary conditions of the PDE.\n",
    "    \"\"\"\n",
    "    x_bc = torch.linspace(x_min, x_max, N_bc)\n",
    "    t_bc = torch.linspace(t_min, t_max, N_bc)\n",
    "    X, T = torch.meshgrid(x_bc, t_bc)\n",
    "\n",
    "    # Make 2D array for initial condition- All x values, t = 0\n",
    "    # Note: X tensor is array to show all values of x and t, Y tensor is U in Heat Equation\n",
    "    left_X = torch.hstack((X[:, 0][:, None], T[:, 0][:, None]))\n",
    "    left_Y = torch.sin(np.pi * left_X[:, 0]).unsqueeze(1)\n",
    "\n",
    "    #Set boundary conditions\n",
    "    #Set left boundary condition for x=x_min at all T\n",
    "    bottom_X = torch.zeros(size = (x_bc.shape[0],2))\n",
    "    bottom_X[:,0] = x_bc.min()\n",
    "    bottom_X[:,1] = t_bc\n",
    "    # bottom_X = torch.hstack((X[0, :][:, None], T[0, :][:, None]))\n",
    "    bottom_Y = torch.zeros(bottom_X.shape[0], 1)\n",
    "    \n",
    "    # Set right boundary condition for x=x_max at all T\n",
    "    top_X = torch.zeros(size = (x_bc.shape[0],2))\n",
    "    top_X[:,0] = x_bc.max()\n",
    "    top_X[:,1] = t_bc\n",
    "    # top_X = torch.hstack((X[-1, :][:, None], T[-1, :][:, None]))\n",
    "\n",
    "    top_Y = torch.zeros(top_X.shape[0], 1)\n",
    "\n",
    "    # Stack all tensors to include boundary conditions and initial conditions\n",
    "    X_train = torch.vstack([left_X, bottom_X, top_X])\n",
    "    Y_train = torch.vstack([left_Y, bottom_Y, top_Y])\n",
    "\n",
    "    # Select random values as part of training process\n",
    "    idx = np.random.choice(X_train.shape[0], N_bc, replace=False)\n",
    "    X_train_BC = X_train[idx, :]\n",
    "    Y_train_BC = Y_train[idx, :]\n",
    "    return X_train_BC, Y_train_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_dataset(x_min,x_max,t_min,t_max,N_test_x,N_test_t):\n",
    "    \"\"\"\n",
    "    This function generates the PyTorch tensors that contain the actual analytical solution to the PDE.\n",
    "    \"\"\"\n",
    "    x = torch.linspace(x_min, x_max, N_test_x).view(-1, 1)\n",
    "    # x = torch.reshape(x,(x.shape[0],1))\n",
    "    t = torch.linspace(t_min, t_max, N_test_t).view(-1, 1)\n",
    "    # t = torch.reshape(t,(t.shape[0],1))\n",
    "\n",
    "    # Create the mesh\n",
    "    X, T = torch.meshgrid(x.squeeze(1),t.squeeze(1))\n",
    "    \n",
    "    # Evaluate solution to PDE\n",
    "    y_real = torch.exp(-T) * (torch.sin(np.pi * X))  # Example 1\n",
    "    # Prepare Data\n",
    "    # Transform the mesh into a 2-column vector to be consistent with other arrays\n",
    "    x_test = torch.hstack((X.transpose(1, 0).flatten().view(-1, 1), T.transpose(1, 0).flatten().view(-1, 1)))\n",
    "    y_test = y_real.view(-1,1).transpose(1, 0).flatten().view(-1, 1)  # Colum major Flatten (so we transpose it)\n",
    "    return y_real, x_test, y_test, X, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class contains all of the methods needed for the neural network. There are several features of this neural network:\n",
    "\n",
    "1) Xavier Normal Initialization: This creates a uniform variance across each layer to prevent the gradient descent method from blowing up.\n",
    "2) Loss functions: This calculates the: 1) difference between predicted values of $U(x,t)$ and the solution to the PDE; 2) and the difference actual predicted values to the boundary conditions and their solution and 3) difference between the predicted initial condition and the actual initial condition. The penalty associated with these differences is the 'loss' due to the entire PDE solution. The neural network aims to minimize this loss and ensure the PDE, boundary condition, and initial condition are satisfied.\n",
    "3) The $L^{2}$ norm is calculated to predict the error of the system.\n",
    "4) ADAM optimization; other optimization methods can be used in lieu of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcn_nn(nn.Module):\n",
    "    def __init__(self,layers, domain):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layers = layers\n",
    "        self.domain = domain\n",
    "\n",
    "        # Activation function to normalize outputs of NN\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # Make loss function a mean squared error\n",
    "        # Note: first argument of loss function is actual data, second argument is empty array?\n",
    "        self.loss_function = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "        \"\"\"Initialise neural network as a list using nn.Modulelist\"\"\"\n",
    "        #Apply linear transformation to data #\n",
    "        self.transf_data = [nn.Linear(self.layers[i], self.layers[i + 1]) for i in range(len(self.layers) - 1)]\n",
    "        self.linear_mod = nn.ModuleList(self.transf_data)\n",
    "        self.optmz = 0 # For optimization\n",
    "\n",
    "        \"Xavier Normal Initialization: Uniform variance across layers to prevent gradient descent from blowing up\"\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            nn.init.xavier_normal_(self.linear_mod[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linear_mod[i].bias.data)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Foward pass function \n",
    "        \"\"\"\n",
    "        if torch.is_tensor(x) != True:\n",
    "            x = torch.from_numpy(x)\n",
    "        a = x.float()\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            z = self.linear_mod[i](a)\n",
    "            a = self.activation(z)\n",
    "        a = self.linear_mod[-1](a)\n",
    "        return a\n",
    "    \n",
    "    \"Loss Functions\"\n",
    "\n",
    "\n",
    "    def lossBC(self, x_BC, y_BC):\n",
    "        \"\"\"\n",
    "        Loss function for Boundary condition\n",
    "        \"\"\"\n",
    "        loss_BC = self.loss_function(self.forward(x_BC), y_BC)\n",
    "        return loss_BC\n",
    "    \n",
    "\n",
    "    def lossPDE(self,x_PDE):\n",
    "        \"\"\"\n",
    "        Loss function for actual PDE solving\n",
    "        \"\"\"\n",
    "        # x_min, x_max, t_min, t_max = self.domain\n",
    "        x = x_PDE.clone()\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        # Calculate U in Diffusion Equation\n",
    "        u = self.forward(x)\n",
    "        u_x_t = autograd.grad(u, x, torch.ones([x.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]  # first derivative of U function with respect to x\n",
    "        u_xx_tt = autograd.grad(u_x_t, x, torch.ones(x.shape).to(device), create_graph=True)[0]  # first derivative of U function with respect to x\n",
    "        u_x = u_x_t[:, [0]]\n",
    "        u_t = u_x_t[:, [1]]  # we select the 2nd element for t (the first one is x) (Remember the input X=[x,t])\n",
    "        u_xx = u_xx_tt[:, [0]]  # we select the 1st element for x (the second one is t) (Remember the input X=[x,t])\n",
    "        u_tt = u_xx_tt[:, [1]]\n",
    "\n",
    "\n",
    "        # f is actual solution to PDE\n",
    "        f = torch.exp(-x[:, 1:]) * (-torch.sin(np.pi * x[:, 0:1]) - np.pi**2 * torch.sin(np.pi * x[:, 0:1]))\n",
    "\n",
    "        #u is loss function- sum of gradients and actual solution\n",
    "        u = u_t - u_xx + f\n",
    "\n",
    "        u_hat = torch.zeros(x.shape[0], 1).to(device)\n",
    "        loss_pde = self.loss_function(u, u_hat)\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x_BC, y_BC, x_PDE, w_bc, w_pde):\n",
    "        \"\"\"\n",
    "        Calculate loss associated with boundary conditions and actual PDE and sums it up with the actual weights\n",
    "        \"\"\"\n",
    "        loss_bc = self.lossBC(x_BC, y_BC)\n",
    "        loss_pde = self.lossPDE(x_PDE)\n",
    "        return w_bc * loss_bc + w_pde * loss_pde\n",
    "    \n",
    "\n",
    "    def error_l2_norm(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate error between predicted data and test data.\n",
    "        \"\"\"\n",
    "        error_u_predict = torch.norm(self.forward(x) - y, dim=1, p=2)\n",
    "        error_u_predict_mean = error_u_predict.mean()\n",
    "        return error_u_predict_mean, error_u_predict\n",
    "    \n",
    "\n",
    "    def relative_error_l2_norm(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate relative error using L2 norm.\n",
    "        \"\"\"\n",
    "        error_u_predict = torch.norm(self.forward(x) - y, dim=1, p=2)\n",
    "        norm_u_real = torch.norm(y, dim=1, p=2)\n",
    "        rela_error = error_u_predict / (norm_u_real)\n",
    "        rela_error_mean = rela_error.mean()\n",
    "        return rela_error_mean, rela_error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the parameters for our PINN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = './pinn_results/'\n",
    "logging.basicConfig(handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(test_folder + \"/0_log.txt\")], level=logging.INFO, format=\"%(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "steps = 3000  # Number of steps in the optimization\n",
    "batch_size = 1000  # Batch size in the optimization\n",
    "w_bc = 100  # Extra weights for the loss_bc functional\n",
    "w_pde = 1  # Extra weights for the loss_pde functional\n",
    "lr = 1e-3  # Learning rate for the stochastic gradient descent\n",
    "layers = np.array([2, 50, 50, 50, 50, 1])  # Number of neurons on each layer of the fully connected neural network\n",
    "log_freq = 50  # log frequency\n",
    "\n",
    "## Set Boundaries for position and time for PDE ##\n",
    "x_min = -1\n",
    "x_max = 1 #L\n",
    "t_min = 0\n",
    "t_max = 1\n",
    "\n",
    "\n",
    "# Define the number of training/test data\n",
    "N_test_x = 200  #  Number of testing points in space @@@ actually\n",
    "N_test_t = 200  #  Number of testing points in time\n",
    "N_train_x = 150  #  Number of training points in space\n",
    "N_train_t = 150  #  Number of training points in time\n",
    "N_bc = 1000  #  Number of training points on the boundary\n",
    "\n",
    "### Generate Training and Test Data ### \n",
    "X_pde = make_pde_data(x_min,x_max,t_min,t_max,N_train_x,N_train_t)\n",
    "X_train_BC, Y_train_BC = make_ic_bc_data(x_min,x_max,t_min,t_max,N_bc)\n",
    "y_real, x_test, y_test, X, T = gen_test_dataset(x_min,x_max,t_min,t_max,N_test_x,N_test_t)\n",
    "\n",
    "# Send data to Device for NN #\n",
    "X_train_BC = X_train_BC.float().to(device)  # Training Points (BC)\n",
    "Y_train_BC = Y_train_BC.float().to(device)  # Training Points (BC)\n",
    "X_pde = X_pde.float().to(device)  # Collocation Points\n",
    "X_test = x_test.float().to(device)  # the input dataset (complete)\n",
    "Y_test = y_test.float().to(device)  # the real solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the neural network and run it to the specified conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Network Model and Optimizer\n",
    "\n",
    "PINN = fcn_nn(layers = layers, domain = [x_min,x_max,t_min,t_max])\n",
    "PINN.to(device)\n",
    "print(PINN)\n",
    "optimizer = torch.optim.Adam(PINN.parameters(), lr=lr, amsgrad=False)\n",
    "\n",
    "# Creating a root directory to store information\n",
    "res_dict = {\"loss\": [], \"loss_bc\": [], \"loss_pde\": [], \"rela_err_l2\": []}\n",
    "start_time_a = time.time()\n",
    "count = 1\n",
    "plot_mesh(X, T, y_real, name=f\"0_real\", error=0, folder=test_folder)  # plot real solution\n",
    "\n",
    "for i in range(1, steps + 1):\n",
    "    idx = np.random.choice(X_pde.shape[0], batch_size, replace=False)  # Choose randomly a batch in x_train_pde\n",
    "    loss = PINN.loss(X_train_BC, Y_train_BC, X_pde[idx, :], w_bc=w_bc, w_pde=w_pde)  # use mean squared error\n",
    "    optimizer.zero_grad()  # We rest the optimizer by making all gradients 0 again\n",
    "    loss.backward()  # Backpropagation (compute the gradients)\n",
    "    optimizer.step()  # Actualization of the parameters\n",
    "    # optimizer.step(PINN.closure) # L-BFGS Optimizer\n",
    "\n",
    "    # Create the plots with the solution with the current parameters, and the relative error.\n",
    "    res_dict[\"loss\"].append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        err_l2_mean, error = PINN.error_l2_norm(X_test, Y_test)\n",
    "        res_dict[\"rela_err_l2\"].append(err_l2_mean.item())\n",
    "    if i % log_freq == 0 or i == 1:\n",
    "        u_predict = PINN(X_test)\n",
    "        arr_y1 = u_predict.reshape(shape=[N_test_t, N_test_x]).transpose(1, 0).detach().cpu()\n",
    "        plot_mesh(X, T, arr_y1, name=f\"approx_{count}\", loss=loss.item(), folder=test_folder, iter=i)\n",
    "        error = error.reshape(shape=[N_test_t, N_test_x]).transpose(1, 0).detach()\n",
    "        plot_mesh(X, T, error, name=f\"error_{count}\", error=err_l2_mean, folder=test_folder, iter=i)\n",
    "        if i % (log_freq*2) == 0 or i == 1:\n",
    "            logger.info(f\"| Iter: {i} | Loss: {loss.item():.4f} | Total_time: {(time.time() - start_time_a)/60:.1f}min\")\n",
    "        count += 1\n",
    "    path = test_folder + \"/A_results_dict.npy\"\n",
    "    np.save(path, np.asarray(res_dict, dtype=object))\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif(test_folder, count, remove_imgs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our results with the intended results. \n",
    "\n",
    "The actual results:\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/real_solution.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "And now our results:\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/final.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "We can see that towards the red-shaded region on the sinusoidal wave ($x > 0$), we lose the definition of $U$. Our calculated loss is decently low to suggest that we have high accuracy for our PINN. Additionally, on my personal machine, it took approximately 135 seconds for the PINN to learn of the solution. Although this is a simple PDE to solve, more complicated PDEs and systems will require more computational resources for which PINNs can be less resource-intensive at its most basic implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
